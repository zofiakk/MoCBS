---
title: "Homework_5"
author: "Kocha≈Ñska Zofia"
date: "27 05 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = TRUE,
  tidy.opts = list(width.cutoff = 95),
  message = FALSE,
  warning = FALSE,
  time_it = TRUE
)
```


# Homework Problem 1
Analyze the 68k PBMCs dataset in the same way as presented in the Seurat's guide with PBMC3k. Apply QC, PCA, jackstraw, clustering, and t-SNE to create figure similar to Figure 3b on Zheng et al. 2017. Note that there are differences between Zheng's original analysis and Seurat's analysis. Pay attentions to hyper-parameters that you must choose for this new bigger dataset.

Provide R markdown file with your codes and outputs.

Present the t-SNE visualization with 10 clusters as defined by K-means clustering

Reproduce Figure 3 but note difference in results: https://www.nature.com/articles/ncomms14049/figures/3

## Setup the Seurat Object

```{r}
library(dplyr)
library(Seurat)
library(patchwork)

# Load the dataset
pbmc.data <- Read10X(data.dir = "./filtered_matrices_mex/hg19/")

# Initialize the Seurat object with the raw (non-normalized data).
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "68kpbmc", min.cells = 3, min.features = 200)
```


## Standard pre-processing workflow

```{r}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
```

```{r}
# Visualize QC metrics as a violin plot
VlnPlot(pbmc, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)


plot1 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2

pbmc <- subset(pbmc, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)
```

## Normalizing the data
```{r normalize}
pbmc <- NormalizeData(pbmc, normalization.method = "LogNormalize", scale.factor = 1e4)
```


## Identification of highly variable features (feature selection)
```{r var_features, fig.height=5, fig.width=11}
pbmc <- FindVariableFeatures(pbmc, selection.method = 'vst', nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(pbmc), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(pbmc)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2
```

## Scaling the data
```{r regressvar, fig.height=7, fig.width=11}
pbmc <- ScaleData(pbmc, vars.to.regress = 'percent.mt')
```


## Perform linear dimensional reduction

```{r pca,results='hide'}
pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
print(pbmc[['pca']], dims = 1:5, nfeatures = 5)
```


```{r pca_viz, message=TRUE}
# Examine and visualize PCA results a few different ways
VizDimLoadings(pbmc, dims = 1:2, reduction = 'pca')
DimPlot(pbmc, reduction = 'pca')
```

```{r single-heatmap}
DimHeatmap(pbmc, dims = 1, cells = 500, balanced = TRUE)
```


```{r multi-heatmap, fig.height=15, fig.width=9}
DimHeatmap(pbmc, dims = 1:15, cells = 500, balanced = TRUE)
```


## Determine the 'dimensionality' of the dataset
```{r jackstraw, fig.height=6, fig.width=10}
# NOTE: This process can take a long time for big datasets, comment out for expediency. More approximate techniques such as those implemented in ElbowPlot() can be used to reduce computation time
pbmc <- JackStraw(pbmc, num.replicate = 25)
pbmc <- ScoreJackStraw(pbmc, dims = 1:20)
```

```{r jsplots, fig.height=6, fig.width=10}
JackStrawPlot(pbmc, dims = 1:15)
```

```{r elbow_plot, fig.height=6, fig.width=10}
ElbowPlot(pbmc)
```


## Cluster the cells

```{r cluster, fig.height=5, fig.width=7}
# from above plots I decided to go with 12 PCs
pbmc <- FindNeighbors(pbmc, dims = 1:12)
pbmc <- FindClusters(pbmc, resolution = 0.5)

# Look at cluster IDs of the first 5 cells
head(Idents(pbmc), 5)
```


## Run non-linear dimensional reduction (tSNE)
```{r tsne, fig.height=5, fig.width=7}
pbmc <- RunTSNE(pbmc, dims = 1:12)
```


```{r tsneplot, fig.height=5, fig.width=7}
library(ggplot2)
pdf("problem1_Kochanska.pdf")
DimPlot(pbmc, reduction = 'tsne', label=TRUE) + ggtitle("tSNE- cells clutered by FindClusters")
dev.off()
```

# Homework Problem 2
Create a hierachical clustering by applying K-means clustering to cells defined by each of 10 cluster. Try to find a suitable number of clusters (k) for each sub-population.

```{r}
pbmc$subcluster <- as.character(Idents(pbmc))

pdf("problem2_1_Kochanska.pdf")
for (indent in 0:(9)){
  cluster <- subset(pbmc, idents = indent)
  cluster <- FindNeighbors(cluster, dims = 1:10)
  cluster <- FindClusters(cluster, resolution = 0.5)
  pbmc$subcluster[Cells(cluster)] <- paste(indent, Idents(cluster), sep = '_')
  print(DimPlot(subset(pbmc, idents = indent), reduction = "tsne", group.by = "subcluster", label = TRUE, label.size = 5) + ggtitle(paste("tSNE sublusters found for cluster: ", indent)))
}
dev.off()
```

```{r}
pdf("problem2_2_Kochanska.pdf")
DimPlot(pbmc, group.by = "subcluster", reduction = "tsne", label = TRUE, label.size = 3) + ggtitle("tSNE- clustered cells with the subcluseters") 
dev.off()
```



